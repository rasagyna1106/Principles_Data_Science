{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students Performance \u2014 Three-Stage Workflow\n\n**Ingest \u2192 Process \u2192 Analyze** \u2014 Step-by-step analysis using StudentsPerformance.csv."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\nimport pandas as pd\nimport numpy as np\n\nRAW_PATH = \"StudentsPerformance.csv\"\nPROCESSED_PATH = os.path.join(\"data\", \"processed\", \"students_performance_processed.csv\")\nREPORTS_DIR = \"reports\"\nos.makedirs(os.path.dirname(PROCESSED_PATH), exist_ok=True)\nos.makedirs(REPORTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Ingest\n\nLoad raw CSV into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.read_csv(RAW_PATH)\ndf.columns = [c.strip().strip('\"') for c in df.columns]\nprint(\"Shape:\", df.shape)\nprint(\"Columns:\", list(df.columns))\ndf.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Data types:\")\ndf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Missing values:\")\ndf.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Process\n\nUnit standardization, feature engineering (total/average score, GradeBand, passed), encoding (test_prep_binary, lunch_standard, gender_male, one-hot GradeBand)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "score_cols = [\"math score\", \"reading score\", \"writing score\"]\nfor c in score_cols:\n    if c in df.columns:\n        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\ndf[\"total_score\"] = df[score_cols].sum(axis=1)\ndf[\"average_score\"] = df[score_cols].mean(axis=1).round(2)\n\ndef score_to_grade(avg):\n    if avg >= 90: return \"A\"\n    elif avg >= 80: return \"B\"\n    elif avg >= 70: return \"C\"\n    elif avg >= 60: return \"D\"\n    else: return \"F\"\ndf[\"GradeBand\"] = df[\"average_score\"].map(score_to_grade)\ndf[\"passed\"] = (df[\"average_score\"] >= 60).astype(int)\ndf[\"test_prep_binary\"] = (df[\"test preparation course\"].str.lower() == \"completed\").astype(\"int8\")\ndf[\"lunch_standard\"] = (df[\"lunch\"].str.lower() == \"standard\").astype(\"int8\")\nif \"gender\" in df.columns:\n    df[\"gender_male\"] = (df[\"gender\"].str.lower() == \"male\").astype(\"int8\")\nif \"GradeBand\" in df.columns:\n    grade_dummies = pd.get_dummies(df[\"GradeBand\"], prefix=\"GradeBand\")\n    df = pd.concat([df, grade_dummies], axis=1)\ndf = df.rename(columns={\"math score\": \"math_score\", \"reading score\": \"reading_score\", \"writing score\": \"writing_score\"})\ndf.to_csv(PROCESSED_PATH, index=False)\nprint(\"Processed data saved to:\", PROCESSED_PATH)\ncols_show = [c for c in [\"math_score\", \"reading_score\", \"writing_score\", \"average_score\", \"GradeBand\", \"passed\", \"test_prep_binary\"] if c in df.columns]\ndf[cols_show].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Analyze & Report\n\nSummary statistics, correlation matrix, key relations \u2192 `reports/findings_students.md`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "numeric_cols = [c for c in df.select_dtypes(include=[np.number]).columns]\nsummary = df[numeric_cols].agg([\"mean\", \"median\", \"std\"]).round(4)\nscore_cols_renamed = [c for c in [\"math_score\", \"reading_score\", \"writing_score\"] if c in df.columns]\nif score_cols_renamed:\n    corr_matrix = df[score_cols_renamed + [\"average_score\", \"passed\", \"test_prep_binary\"]].corr()\nelse:\n    corr_matrix = df[numeric_cols].corr()\ncorr_avg_passed = df[\"average_score\"].corr(df[\"passed\"]) if \"passed\" in df.columns else None\ncorr_avg_testprep = df[\"average_score\"].corr(df[\"test_prep_binary\"]) if \"test_prep_binary\" in df.columns else None\n\ndef df_to_md(d):\n    return \"```\\n\" + d.to_string() + \"\\n```\"\nfindings = \"# Students Performance \u2014 Findings\\n\\n## 1. Summary statistics (numeric columns)\\n\\n\"\nfindings += df_to_md(summary)\nfindings += \"\\n\\n## 2. Correlation matrix (scores and key binaries)\\n\\n\"\nfindings += df_to_md(corr_matrix.round(4))\nfindings += \"\\n\\n## 3. Key relations\\n\\n\"\nif corr_avg_passed is not None:\n    findings += f\"- **Correlation(average_score, passed):** {corr_avg_passed:.4f}\\n\"\nif corr_avg_testprep is not None:\n    findings += f\"- **Correlation(average_score, test_prep_binary):** {corr_avg_testprep:.4f}\\n\"\nfindings += \"\\nInterpretation: Positive correlation with test_prep_binary suggests completing test preparation is associated with higher average scores.\\n\"\nout_path = os.path.join(REPORTS_DIR, \"findings_students.md\")\nwith open(out_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(findings)\nprint(summary.head())\nprint(\"\\nCorrelation(average_score, passed):\", corr_avg_passed)\nprint(\"Correlation(average_score, test_prep_binary):\", corr_avg_testprep)\nprint(\"\\nReport written to:\", out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}